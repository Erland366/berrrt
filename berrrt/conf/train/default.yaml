num_train_epochs: 5
optim: adamw_torch_fused
learning_rate: 1e-5
per_device_train_batch_size: 32
per_device_eval_batch_size: 64
warmup_steps: 500
weight_decay: 0.01
logging_dir: ./logs
logging_steps: 100
evaluation_strategy: epoch
save_strategy: epoch
load_best_model_at_end: true
metric_for_best_model: "accuracy"
report_to: wandb
