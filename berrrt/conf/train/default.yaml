num_train_epochs: 100
optim: adamw_torch
per_device_train_batch_size: 32
per_device_eval_batch_size: 64
learning_rate: 1e-3
warmup_steps: 500
weight_decay: 0.01
logging_dir: ./logs
logging_steps: 20
evaluation_strategy: steps
report_to: wandb
