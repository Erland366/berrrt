num_train_epochs: 50
optim: adamw_torch_fused
per_device_train_batch_size: 32
per_device_eval_batch_size: 64
learning_rate: 5e-5
warmup_steps: 500
weight_decay: 0.01
logging_dir: ./logs
logging_steps: 100
evaluation_strategy: steps
report_to: wandb
