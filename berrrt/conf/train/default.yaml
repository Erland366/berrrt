num_train_epochs: 30
optim: adamw_torch
per_device_train_batch_size: 64
per_device_eval_batch_size: 32
learning_rate: 2e-5
logging_dir: ./logs
logging_steps: 20
eval_steps: 100
evaluation_strategy: steps
push_to_hub: False
metric_for_best_model: accuracy
load_best_model_at_end: True
report_to: "wandb"
use_cpu: False
save_total_limit: 1