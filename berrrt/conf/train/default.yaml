num_train_epochs: 3
optim: adamw_torch
per_device_train_batch_size: 32
per_device_eval_batch_size: 64
learning_rate: 2e-5
logging_dir: ./logs
logging_steps: 100
evaluation_strategy: steps
report_to: wandb
